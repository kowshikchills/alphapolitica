{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "packages\n",
    "'''\n",
    "import pandas as pd \n",
    "import ast\n",
    "from googletrans import Translator\n",
    "from multiprocessing import Pool, Manager\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Translation of transcibing file\n",
    "'''\n",
    "df_transciption = pd.read_csv('data/complete_data_transcribing.csv')\n",
    "df_transciption = df_transciption[df_transciption.sentences != 'failed']\n",
    "df_transciption['ids'] = df_transciption['id'] \n",
    "df_transciption['comp'] = np.arange(len(df_transciption))\n",
    "del df_transciption['Unnamed: 0']\n",
    "translator = Translator()\n",
    "def update_data_with_trans(A):\n",
    "    d = A[0]\n",
    "    i = A[1]\n",
    "    extracted =  df_transciption[df_transciption.comp == i]['sentences'].values[0]\n",
    "    d[i] = translator.translate(extracted, dest='en').text\n",
    "\n",
    "manager = Manager()\n",
    "translated_dict = manager.dict()\n",
    "with Pool(15) as p:\n",
    "    p.map(update_data_with_trans, [[translated_dict,i] for i in np.arange(len(df_transciption))])\n",
    "df_transciption['translated'] = [translated_dict[i] for i in np.arange(len(df_transciption))]\n",
    "del df_transciption['comp']\n",
    "del df_transciption['ids']\n",
    "df_transciption.to_csv('data/complete_data_transcribing_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Translation of attributes file \n",
    "'''\n",
    "\n",
    "df_attributes = pd.read_csv('data/videos_alphapolitica_complete_info_ids.csv')\n",
    "lis_tag = [ast.literal_eval(i) for i in df_attributes['tags']]\n",
    "lis_tags = [j for i in lis_tag for j in i]\n",
    "dict_counter = dict(Counter(lis_tags))\n",
    "df_counter = pd.DataFrame()\n",
    "df_counter['tags'] = dict_counter.keys()\n",
    "df_counter['counter'] = dict_counter.values()\n",
    "df_counter = df_counter.sort_values(by='counter', ascending= False)\n",
    "stopwords = [i.lower() for i in df_counter[df_counter.counter > 7000]['tags'].values]\n",
    "to_drop = ['today ap news', 'today telugu news','tv5 telugu live','ap news live','ap political news']\n",
    "stopwords = stopwords + to_drop\n",
    "df_attributes['tags'] = [ast.literal_eval(i) for i in df_attributes['tags']]\n",
    "df_attributes['tags_updated'] = [list(set([j.lower() for j in i ])-set(stopwords)) for i in  df_attributes['tags'].values]\n",
    "df_attributes['tags'] = [' '.join(i) for i in df_attributes['tags_updated']]\n",
    "del df_attributes['tags_updated']\n",
    "del df_attributes['Unnamed: 0']\n",
    "df_attributes = df_attributes.dropna(subset=['title'])\n",
    "df_attributes['tags'] = [' '.join(ast.literal_eval(i)) for i in df_attributes['tags']]\n",
    "df_attributes = df_attributes[['ids','title','tags','upload_date','duration','comment_count','view_count','uploader']]\n",
    "df_attributes['comp'] = np.arange(len(df_attributes))\n",
    "translator = Translator()\n",
    "def update_data_with_trans(A):\n",
    "    d = A[0]\n",
    "    i = A[1]\n",
    "    extracted =  df_attributes[df_attributes.comp == i]['title'].values[0]\n",
    "    d[i] = translator.translate(extracted, dest='en').text\n",
    "\n",
    "manager = Manager()\n",
    "translated_dict = manager.dict()\n",
    "with Pool(15) as p:\n",
    "    p.map(update_data_with_trans, [[translated_dict,i] for i in np.arange(len(df_attributes))])\n",
    "df_attributes['title_translated'] = [translated_dict[i] for i in np.arange(len(df_attributes))]\n",
    "del df_attributes['comp']\n",
    "df_attributes.to_csv('data/videos_alphapolitica_complete_info_ids_translated.csv', index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/videos_alphapolitica_complete_info_ids_translated_update.csv')\n",
    "df_transciption = pd.read_csv('data/complete_data_transcribing_translated.csv')\n",
    "df_attributes = pd.read_csv('data/videos_alphapolitica_complete_info_ids_translated_update.csv')\n",
    "df_attributes_merged = df_attributes.merge(df_transciption, on='ids', how='outer')\n",
    "df_attributes_merged = df_attributes_merged.dropna(subset = ['title'])\n",
    "df_attributes_merged = df_attributes_merged[['ids','uploader','translated','tags_translated','title_translated','upload_date','duration','comment_count','view_count']]\n",
    "df_attributes_merged.to_csv('data/data_compiled_final_may_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Data is created, paraphrasing ! \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kowshik/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=1280\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "df_data  = pd.read_csv('data_compiled_final_may_2023.csv')\n",
    "df_data['translated'] =  df_data['translated'].fillna('')\n",
    "\n",
    "translated_paraphrase = []\n",
    "for i in tqdm(range(len(df_data))):\n",
    "    text = df_data['translated'].values[i]\n",
    "    if text == '':\n",
    "        translated_paraphrase.append(text)\n",
    "    else:\n",
    "        translated_paraphrase.append(paraphrase(text)[0])\n",
    "\n",
    "df_data['translated_paraphrase'] = translated_paraphrase\n",
    "df_data.to_csv('data_compiled_final_may_2023_paraphrased.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Classification\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kowshik/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data  = pd.read_csv('data/data_compiled_final_may_2023.csv')\n",
    "df_data['translated'] =  df_data['translated'].fillna('')\n",
    "df_data['tags_translated'] =  df_data['tags_translated'].fillna('')\n",
    "df_data['title_translated'] =  df_data['title_translated'].fillna('')\n",
    "df_data['sentence_for_classification'] = df_data['title_translated'] + ' ' + df_data['translated'] + ' ' +  df_data['tags_translated'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-mnli\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pipe(df_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_for_classification\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m11\u001b[39m],candidate_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschemes\u001b[39m\u001b[38;5;124m\"\u001b[39m],device \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/pipelines/__init__.py:754\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    753\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 754\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    755\u001b[0m     model,\n\u001b[1;32m    756\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    757\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    758\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    759\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    760\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    761\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    762\u001b[0m )\n\u001b[1;32m    764\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    765\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/pipelines/base.py:257\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    259\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/modeling_utils.py:2478\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2468\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2469\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2471\u001b[0m     (\n\u001b[1;32m   2472\u001b[0m         model,\n\u001b[1;32m   2473\u001b[0m         missing_keys,\n\u001b[1;32m   2474\u001b[0m         unexpected_keys,\n\u001b[1;32m   2475\u001b[0m         mismatched_keys,\n\u001b[1;32m   2476\u001b[0m         offload_index,\n\u001b[1;32m   2477\u001b[0m         error_msgs,\n\u001b[0;32m-> 2478\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2479\u001b[0m         model,\n\u001b[1;32m   2480\u001b[0m         state_dict,\n\u001b[1;32m   2481\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2482\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2483\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2484\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2485\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2486\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2487\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2488\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2489\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2490\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2491\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2492\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[1;32m   2493\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2494\u001b[0m     )\n\u001b[1;32m   2496\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n\u001b[1;32m   2498\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/modeling_utils.py:2748\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2739\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   2740\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   2741\u001b[0m         state_dict,\n\u001b[1;32m   2742\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   2747\u001b[0m     )\n\u001b[0;32m-> 2748\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   2749\u001b[0m     offload_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2751\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m \n\u001b[1;32m   2753\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/modeling_utils.py:491\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 491\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[1;32m    492\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/modeling_utils.py:489\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/modeling_utils.py:489\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/transformers/modeling_utils.py:485\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    483\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    484\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    487\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/personal/alphapolitica/alphapenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1942\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1942\u001b[0m         param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[1;32m   1943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   1944\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1945\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1946\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m                       \u001b[39m.\u001b[39mformat(key, param\u001b[39m.\u001b[39msize(), input_param\u001b[39m.\u001b[39msize(), ex\u001b[39m.\u001b[39margs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "pipe(df_data['sentence_for_classification'].values[11],candidate_labels=[\"schemes\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['chandhra babu','YCP', 'jagan', 'TDP','murder',\n",
    " 'vivek', 'roads', 'capital', 'atrocities','volunteer', 'amma vadi',\n",
    "  'welfare', 'schemes', 'lokesh', 'pattabi', 'vijayawada', 'visaka',\n",
    "   'jogi ramesh','vamsi','kodali','nani', 'Education','Agriculture',\n",
    "   'farmers','Corruption','investment','Budget','SC/ST','Polavaram',\n",
    "   'irrigation','YSRCP','Telugu Desam Party','Vidya Deevena','education',\n",
    "   'governance', ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device = 'cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = ['chandhra babu','YCP', 'jagan', 'TDP','murder', 'vivek', 'roads', 'capital', 'atrocities','volunteer', 'amma vadi', 'welfare', 'schemes', 'lokesh', 'pattabi', 'vijayawada', 'visaka', 'jogi ramesh','vamsi','kodali','nani']\n",
    "predictions = {}\n",
    "for topic in topics:\n",
    "    predictions[topic] = []\n",
    "    sentences = df_one_shot['to_pred'].values\n",
    "    for sent in tqdm(sentences):\n",
    "        out = classifier(sent, ['chandhra babu'])\n",
    "        predictions[topic].append(out['scores'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_para['para_phrased'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "results = nlp(sentences)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('alphapenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "277fab76d0acca917d9ff45691135e5555c82671ab771a82f606bf05d4140469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
